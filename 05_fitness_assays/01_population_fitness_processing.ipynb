{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YB8msd0X0GyZ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import csv\n","import glob\n","import statsmodels.formula.api as smf"]},{"cell_type":"markdown","metadata":{"id":"8bwBH9hGC8l2"},"source":["# FA01\n","Fitness assays of populations in YPD.\n","\n","Going from `t1_data.txt, t2_data.txt` to `FA01_parsed_fitness_data.txt`.\n","\n","Needs `plate_layout_FA01.txt`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81,"status":"ok","timestamp":1770187422424,"user":{"displayName":"Shreyas Pai","userId":"15597218115471730073"},"user_tz":300},"id":"fUrhUyaKyFRX","outputId":"9fbb1cb4-1f2c-45e4-d1de-1ac9f9b643bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Transformed data saved to t1f_parsed.txt\n","Transformed data saved to t2f_parsed.txt\n"]}],"source":["## t1/2_data.txt -> t1/2f_parsed.txt\n","\n","def transform_data(input_file, output_file):\n","    # Load the input file\n","    data = pd.read_csv(input_file, sep=\"\\t\", names=[\"Name\", \"Statistic\", \"Cells\"])\n","\n","    # Extract the base file names\n","    data[\"BaseName\"] = data[\"Name\"].str.extract(r\"(^.*\\.fcs)\")\n","\n","    # Filter rows for each metric\n","    total_counts = data[data[\"Name\"].str.endswith(\".fcs\")]\n","    cells_counts = data[data[\"Name\"].str.endswith(\"/cells\")]\n","    mcherry_counts = data[data[\"Name\"].str.endswith(\"/cells/mCherry\")]\n","\n","    # Merge the counts based on the base file name\n","    merged = pd.merge(total_counts[[\"BaseName\", \"Cells\"]], cells_counts[[\"BaseName\", \"Cells\"]], on=\"BaseName\", suffixes=(\"_total\", \"_cells\"))\n","    merged = pd.merge(merged, mcherry_counts[[\"BaseName\", \"Cells\"]], on=\"BaseName\")\n","    merged.rename(columns={\"Cells\": \"Cells_mCherry\"}, inplace=True)\n","\n","    # Save to output\n","    merged[[\"BaseName\", \"Cells_total\", \"Cells_cells\", \"Cells_mCherry\"]].to_csv(output_file, sep=\"\\t\", index=False, header=False)\n","\n","    print(f\"Transformed data saved to {output_file}\")\n","\n","# Process t1_data.txt and t2_data.txt\n","transform_data(\"t1_data.txt\", \"t1f_parsed.txt\")\n","transform_data(\"t2_data.txt\", \"t2f_parsed.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1770187422445,"user":{"displayName":"Shreyas Pai","userId":"15597218115471730073"},"user_tz":300},"id":"jF0mGJ0I0T7d","outputId":"e4b850b5-c1c9-4fe6-9a07-ac478c0e2468"},"outputs":[{"name":"stdout","output_type":"stream","text":["Winfo file saved to winfo_t1f_parsed.txt\n","Winfo file saved to winfo_t2f_parsed.txt\n"]}],"source":["## t1/2f_parsed.txt -> winfo_t1/2f_parsed.txt\n","\n","def transform_parsed_to_winfo(parsed_file, plate_layout_file, output_file):\n","    \"\"\"\n","    Transform parsed file to winfo format using plate layout with row and well repetition.\n","\n","    Args:\n","        parsed_file (str): Path to the parsed file (e.g., t1f_parsed.txt).\n","        plate_layout_file (str): Path to the plate_layout.txt file.\n","        output_file (str): Path to save the transformed winfo file.\n","    \"\"\"\n","    # Load parsed data\n","    parsed_data = pd.read_csv(parsed_file, sep=\"\\t\", header=None, names=[\"V1\", \"V2\", \"V3\", \"V4\"])\n","\n","    # Load plate layout\n","    plate_layout = pd.read_csv(plate_layout_file, sep=\"\\t\", header=None)\n","\n","    # Repeat each well twice consecutively\n","    repeated_wells = []\n","    for row in plate_layout.values:\n","        repeated_row = []\n","        for well in row:\n","            repeated_row.extend([well, well])  # Repeat each well twice\n","        repeated_wells.append(repeated_row)\n","        repeated_wells.append(repeated_row)  # Repeat each row twice\n","\n","    # Flatten the plate layout for `sample.id`\n","    sample_ids = [well for row in repeated_wells for well in row]\n","\n","    # Adjust the length of sample IDs to match parsed data\n","    sample_ids = sample_ids * (len(parsed_data) // len(sample_ids))\n","    parsed_data[\"sample.id\"] = sample_ids\n","\n","    # Generate the 'fact' column to match winfo files - not sure how this was actually generated?\n","    facts = []\n","    for v1 in parsed_data[\"V1\"]:\n","        # Extract the row letter from the specimen name (e.g., \"A\", \"B\", etc.)\n","        row_letter = v1.split(\"_\")[2][0]\n","\n","        # Assign fact based on row letter (A/C/E/G -> A/B alternating, B/D/F/H -> C/D alternating)\n","        if row_letter in [\"A\", \"C\", \"E\", \"G\", \"I\", \"K\", \"M\", \"O\"]:\n","            fact = \"A\" if len(facts) % 2 == 0 else \"B\"\n","        elif row_letter in [\"B\", \"D\", \"F\", \"H\", \"J\", \"L\", \"N\", \"P\"]:\n","            fact = \"C\" if len(facts) % 2 == 0 else \"D\"\n","        facts.append(fact)\n","    parsed_data[\"fact\"] = facts\n","\n","    # Save the transformed data to output file\n","    parsed_data.to_csv(output_file, sep=\"\\t\", index=False)\n","    print(f\"Winfo file saved to {output_file}\")\n","\n","# Example usage\n","transform_parsed_to_winfo(\"t1f_parsed.txt\", \"plate_layout_FA01.txt\", \"winfo_t1f_parsed.txt\")\n","transform_parsed_to_winfo(\"t2f_parsed.txt\", \"plate_layout_FA01.txt\", \"winfo_t2f_parsed.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1770187422461,"user":{"displayName":"Shreyas Pai","userId":"15597218115471730073"},"user_tz":300},"id":"6IyoAoB-XGTM","outputId":"e45570de-ad65-4bee-e378-38d5a45ef7bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copied winfo_t1f_parsed.txt to FA01_YPD_t1.txt\n","Copied winfo_t2f_parsed.txt to FA01_YPD_t2.txt\n"]}],"source":["## renaming winfo_t1/2f_parsed.txt -> FA01_YPD_t1/2.txt\n","\n","def copy_files():\n","    # Define the mapping of old file names to new file names\n","    file_mapping = {\n","        \"winfo_t1f_parsed.txt\": \"FA01_YPD_t1.txt\",\n","        \"winfo_t2f_parsed.txt\": \"FA01_YPD_t2.txt\",\n","    }\n","\n","    for old_name, new_name in file_mapping.items():\n","        if os.path.exists(old_name):\n","            with open(old_name, 'rb') as src_file:\n","                with open(new_name, 'wb') as dest_file:\n","                    dest_file.write(src_file.read())\n","            print(f\"Copied {old_name} to {new_name}\")\n","        else:\n","            print(f\"File {old_name} not found.\")\n","\n","if __name__ == \"__main__\":\n","    copy_files()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1770187422497,"user":{"displayName":"Shreyas Pai","userId":"15597218115471730073"},"user_tz":300},"id":"zOsTH5FPmUnh","outputId":"9bf1c219-c361-4020-c93b-fbe3b145a7a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Haploid data saved to FA01_parsed_fitness_data.txt\n","Diploid data saved to FA01_parsed_fitness_data_2N.txt\n"]}],"source":["## FA01_YPD_t1/2.txt -> FA01_parsed_fitness_data.txt and FA01_parsed_fitness_data_2N.txt\n","\n","def fit_est(f1, f0, t):\n","    \"\"\"\n","    Calculate the fitness estimate using the given formula.\n","    \"\"\"\n","    return (1 / t) * (np.log(f1 / (1 - f1)) - np.log(f0 / (1 - f0)))\n","\n","# Helper function to safely split sample_id\n","def safe_split(sample_id, index):\n","    parts = sample_id.split(\"_\")\n","    return parts[index] if len(parts) > index else \"NA\"\n","\n","# Load data\n","YPD_t1 = pd.read_csv(\"FA01_YPD_t1.txt\", sep=\"\\t\")\n","YPD_t2 = pd.read_csv(\"FA01_YPD_t2.txt\", sep=\"\\t\")\n","\n","# Merge data\n","YPD = pd.merge(YPD_t1, YPD_t2.iloc[:, :4], on=\"V1\", how=\"left\")\n","YPD[\"well_id\"] = YPD[\"V1\"].apply(lambda x: x.split(\"_\")[3].replace(\".fcs\", \"\"))\n","YPD = YPD.drop(columns=[\"V1\"])\n","YPD.columns = [\"tot_t1\", \"cells_t1\", \"ref_t1\", \"sample_id\", \"plate\", \"tot_t2\", \"cells_t2\", \"ref_t2\", \"well_id\"]\n","\n","# Add additional metadata with error handling\n","YPD[\"sex_cycle\"] = YPD[\"sample_id\"].apply(lambda x: safe_split(x, 1))\n","YPD[\"kk_well_id\"] = YPD[\"sample_id\"].apply(lambda x: safe_split(x, 2))\n","\n","# Flag regime\n","YPD[\"regime\"] = np.where(YPD[\"sample_id\"].str.contains(\"a1.1_|2N.1_\"), \"asexual\", \"NA\")\n","YPD.loc[YPD[\"sample_id\"].str.contains(\"a3_|2N.3_\"), \"regime\"] = \"sexual\"\n","\n","# Flag ancestor\n","YPD[\"anc\"] = np.where(YPD[\"sample_id\"].str.contains(\"aAnc|dipAnc\"), 1, 0)\n","\n","# Flag diploids\n","YPD[\"diploid\"] = np.where(YPD[\"sample_id\"].str.startswith(\"2N\") | YPD[\"sample_id\"].str.startswith(\"dip\"), 1, 0)\n","\n","# Flag reference\n","YPD[\"ref\"] = np.where(YPD[\"sample_id\"].str.endswith(\"FR\"), 1, 0)\n","\n","# Add assay\n","YPD[\"assay\"] = \"FA01\"\n","\n","# Estimate fitness\n","YPD[\"s_hat\"] = fit_est(\n","    f1=(YPD[\"cells_t2\"] - YPD[\"ref_t2\"]) / YPD[\"cells_t2\"],\n","    f0=(YPD[\"cells_t1\"] - YPD[\"ref_t1\"]) / YPD[\"cells_t1\"],\n","    t=10,\n",")\n","YPD[\"s_hat\"] = YPD[\"s_hat\"].astype(object)  # Convert to object for mixed values\n","YPD.loc[YPD[\"ref\"] == 1, \"s_hat\"] = \"NA\"\n","\n","# Add environment and replicate\n","YPD[\"env\"] = \"YPD\"\n","YPD[\"rep\"] = YPD[\"plate\"]\n","YPD[\"plate\"] = \"YPD_\" + YPD[\"plate\"]\n","\n","# Replace blanks with NAs\n","YPD = YPD.fillna(\"NA\")\n","\n","# Export haploid data\n","relcols = [\"kk_well_id\", \"assay\", \"regime\", \"plate\", \"env\", \"rep\", \"anc\", \"ref\", \"s_hat\"]\n","YPD_haps = YPD[YPD[\"diploid\"] == 0][relcols]\n","\n","# Remove outlier samples\n","YPD_haps = YPD_haps[YPD_haps[\"s_hat\"].astype(str) != \"NA\"]\n","YPD_haps = YPD_haps[YPD_haps[\"s_hat\"].astype(float) < 0.1]\n","\n","# Add kk_pop_id with NA for missing values\n","YPD_haps[\"kk_pop_id\"] = \"NA\"\n","YPD_haps.loc[\n","    (YPD_haps[\"regime\"] == \"sexual\") & (YPD_haps[\"anc\"] == 0), \"kk_pop_id\"\n","] = YPD_haps[\"kk_well_id\"].apply(lambda x: f\"a3_{x}\")\n","YPD_haps.loc[\n","    (YPD_haps[\"regime\"] == \"asexual\") & (YPD_haps[\"anc\"] == 0), \"kk_pop_id\"\n","] = YPD_haps[\"kk_well_id\"].apply(lambda x: f\"a1.1_{x}\")\n","\n","# Define the custom header\n","header = [\n","    '\"kk_well_id\"',\n","    '\"assay\"',\n","    '\"regime\"',\n","    '\"plate\"',\n","    '\"env\"',\n","    '\"rep\"',\n","    '\"anc\"',\n","    '\"ref\"',\n","    '\"s_hat\"',\n","    '\"kk_pop_id\"',\n","]\n","\n","# Format the output to ensure correct quoting\n","def format_row(row):\n","    return [\n","        f'\"{val}\"' if isinstance(val, str) and val != \"NA\" else str(val)\n","        for val in row\n","    ]\n","\n","# Save haploid data with the specified header\n","with open(\"FA01_parsed_fitness_data.txt\", \"w\") as f:\n","    f.write(\",\".join(header) + \"\\n\")  # Write the header\n","    for _, row in YPD_haps.iterrows():\n","        formatted_row = format_row(row)\n","        f.write(\",\".join(formatted_row) + \"\\n\")\n","\n","print(\"Haploid data saved to FA01_parsed_fitness_data.txt\")\n","\n","## DIPLOIDS - diploid data but this won't be used for any subsequent analyses for this project\n","\n","# Save diploid data with dynamic headers derived from YPD\n","diploid_data = YPD[YPD[\"diploid\"] == 1]  # Filter for diploid data\n","\n","# Format column names dynamically for diploid export\n","diploid_header = [f'\"{col}\"' for col in diploid_data.columns]\n","\n","# Save diploid data with dynamically derived headers\n","with open(\"FA01_parsed_fitness_data_2N.txt\", \"w\") as f:\n","    f.write(\",\".join(diploid_header) + \"\\n\")  # Write dynamic header\n","    for _, row in diploid_data.iterrows():\n","        formatted_row = [\n","            f'\"{val}\"' if isinstance(val, str) and val != \"NA\" else str(val)\n","            for val in row\n","        ]\n","        f.write(\",\".join(formatted_row) + \"\\n\")\n","\n","print(\"Diploid data saved to FA01_parsed_fitness_data_2N.txt\")"]},{"cell_type":"markdown","metadata":{"id":"HXQuZ1RODGUA"},"source":["# FA02\n","Fitness assays of populations in SC 30C, SC 37C, SC +NaCl, SC pH 7.3, SC -P.\n","\n","Going from `1AB2AB_t1.txt, 1AB2AB_t2.txt, 3AB4AB_t1.txt, 3AB4AB_t2.txt, 5AB6AB_t1.txt, 5AB6AB_t2.txt` to `FA02_parsed_fitness_data.txt`.\n","\n","Needs `plate_layout_FA02.txt`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1770187424363,"user":{"displayName":"Shreyas Pai","userId":"15597218115471730073"},"user_tz":300},"id":"Ee8Az4OtDRG4","outputId":"634d43cb-117f-4bdd-b768-0cd9121756fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished processing 1AB2AB_t1.txt, output saved to parsed_1AB2AB_t1.txt.\n","Finished processing 1AB2AB_t2.txt, output saved to parsed_1AB2AB_t2.txt.\n","Finished processing 3AB4AB_t1.txt, output saved to parsed_3AB4AB_t1.txt.\n","Finished processing 3AB4AB_t2.txt, output saved to parsed_3AB4AB_t2.txt.\n","Finished processing 5AB6AB_t1.txt, output saved to parsed_5AB6AB_t1.txt.\n","Finished processing 5AB6AB_t2.txt, output saved to parsed_5AB6AB_t2.txt.\n"]}],"source":["## 1AB2AB_t1/2.txt -> parsed_1AB2AB_t1/2.txt etc\n","\n","files_to_process = [\n","    \"1AB2AB_t1.txt\",\n","    \"1AB2AB_t2.txt\",\n","    \"3AB4AB_t1.txt\",\n","    \"3AB4AB_t2.txt\",\n","    \"5AB6AB_t1.txt\",\n","    \"5AB6AB_t2.txt\"\n","]\n","\n","def parse_file(input_file):\n","    tot_lines = []\n","    ref_lines = []\n","\n","    # Read and classify lines\n","    with open(input_file, 'r') as f:\n","        for line in f:\n","            if '.fcs' in line and '/mCherry' not in line:\n","                tot_lines.append(line.strip())\n","            elif '/mCherry' in line:\n","                ref_lines.append(line.strip())\n","\n","    # Check if the number of lines matches\n","    if len(tot_lines) != len(ref_lines):\n","        print(f\"Mismatch in line counts! Total: {len(tot_lines)}, Reference: {len(ref_lines)}\")\n","        return\n","\n","    # Combine data and write parsed output\n","    parsed_file = f\"parsed_{input_file}\"\n","    with open(parsed_file, 'w') as out:\n","        for tot, ref in zip(tot_lines, ref_lines):\n","            tot_parts = re.split(r'\\s+', tot.strip())\n","            ref_parts = re.split(r'\\s+', ref.strip())\n","\n","            if len(tot_parts) >= 2 and len(ref_parts) >= 3:\n","                out.write(f\"{tot_parts[0]}\\t{tot_parts[1]}\\t{ref_parts[2]}\\n\")\n","\n","    print(f\"Finished processing {input_file}, output saved to {parsed_file}.\")\n","\n","# Process specified files\n","for file_name in files_to_process:\n","    if os.path.exists(file_name):  # Check if the file exists\n","        parse_file(file_name)\n","    else:\n","        print(f\"File {file_name} not found, skipping.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1770187424487,"user":{"displayName":"Shreyas Pai","userId":"15597218115471730073"},"user_tz":300},"id":"oo-Fbr0VDU2W","outputId":"19c917c8-a531-4df4-a51d-a45e993bf0d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final parsed file saved to final_parsed_1AB2AB_t1.txt\n","Final parsed file saved to final_parsed_1AB2AB_t2.txt\n","Final parsed file saved to final_parsed_3AB4AB_t1.txt\n","Final parsed file saved to final_parsed_3AB4AB_t2.txt\n","Final parsed file saved to final_parsed_5AB6AB_t1.txt\n","Final parsed file saved to final_parsed_5AB6AB_t2.txt\n"]}],"source":["## parsed_1AB2AB_t1/2.txt -> final_parsed_1AB2AB_t1/2.txt etc\n","\n","def add_missing_well_ids(parsed_data):\n","    \"\"\"\n","    Identifies and adds missing well_ids based on the repeating pattern A01-A24, B01-B24, etc.\n","\n","    Args:\n","        parsed_data (pd.DataFrame): The DataFrame containing the parsed data with assigned well_ids.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with missing well_ids added.\n","    \"\"\"\n","    # Define the expected well_id pattern\n","    rows = \"ABCDEFGHIJKLMNOP\"\n","    cols = [f\"{i:02d}\" for i in range(1, 25)]\n","    expected_well_ids = [f\"{row}{col}\" for row in rows for col in cols]\n","\n","    # Identify missing well_ids\n","    existing_well_ids = parsed_data[\"well_id\"].tolist()\n","    missing_well_ids = set(expected_well_ids) - set(existing_well_ids)\n","\n","    # Create a DataFrame for the missing well_ids with all other columns as NaN\n","    missing_data = pd.DataFrame({\n","        \"well_id\": list(missing_well_ids),\n","        \"sample_id\": [pd.NA] * len(missing_well_ids),\n","        \"fact\": [pd.NA] * len(missing_well_ids),\n","        \"name_long\": [pd.NA] * len(missing_well_ids),\n","        \"cells\": [pd.NA] * len(missing_well_ids),\n","        \"ref\": [pd.NA] * len(missing_well_ids)\n","    })\n","\n","    # Combine the original and missing data, then sort by well_id\n","    combined_data = pd.concat([parsed_data, missing_data]).sort_values(\"well_id\").reset_index(drop=True)\n","    return combined_data\n","\n","def extract_fact_prefix(filename):\n","    \"\"\"\n","    Extracts the fact prefix (1, 3, 5, etc.) from the filename.\n","\n","    Args:\n","        filename (str): The input filename.\n","\n","    Returns:\n","        int: The fact prefix (e.g., 1, 3, 5).\n","    \"\"\"\n","    if \"1AB2AB\" in filename:\n","        return 1\n","    elif \"3AB4AB\" in filename:\n","        return 3\n","    elif \"5AB6AB\" in filename:\n","        return 5\n","    else:\n","        raise ValueError(f\"Unexpected filename format: {filename}\")\n","\n","def transform_parsed_to_final(parsed_file, plate_layout_file, output_file):\n","    \"\"\"\n","    Transform parsed data to final parsed format using the plate layout.\n","\n","    Args:\n","        parsed_file (str): Path to the parsed file.\n","        plate_layout_file (str): Path to the plate_layout.txt file.\n","        output_file (str): Path to save the transformed final parsed file.\n","    \"\"\"\n","    # Load parsed data\n","    parsed_data = pd.read_csv(parsed_file, sep=\"\\t\", header=None, names=[\"name_long\", \"cells\", \"ref\"])\n","\n","    # Extract well_id from name_long (e.g., \"Specimen_001_A1_A01.fcs\" -> \"A01\")\n","    parsed_data[\"well_id\"] = parsed_data[\"name_long\"].str.extract(r\"(?:.*?_){3}([^_.]+)\")\n","\n","    # Add missing well_ids\n","    parsed_data = add_missing_well_ids(parsed_data)\n","\n","    # Load plate layout\n","    plate_layout = pd.read_csv(plate_layout_file, sep=\"\\t\", header=None)\n","\n","    # Repeat each well and each row twice for sample_id\n","    repeated_wells = []\n","    for row in plate_layout.values:\n","        repeated_row = []\n","        for well in row:\n","            repeated_row.extend([well, well])  # Repeat each well twice\n","        repeated_wells.append(repeated_row)\n","        repeated_wells.append(repeated_row)  # Repeat each row twice\n","\n","    # Flatten the repeated wells\n","    repeated_sample_ids = [well for row in repeated_wells for well in row]\n","\n","    # Ensure sample_id length matches parsed data\n","    repeat_factor = len(parsed_data) // len(repeated_sample_ids) + 1\n","    repeated_sample_ids = np.tile(repeated_sample_ids, repeat_factor)[:len(parsed_data)]\n","    parsed_data[\"sample_id\"] = repeated_sample_ids\n","\n","    # Extract fact prefix from the filename\n","    fact_prefix = extract_fact_prefix(parsed_file)\n","\n","    # Assign `fact` column based on row letters and alternation\n","    facts = []\n","    for i, well in enumerate(parsed_data[\"well_id\"]):\n","        if pd.isna(well):  # Handle NaN cases for added rows\n","            facts.append(pd.NA)\n","            continue\n","        row_letter = well[0]\n","        if row_letter in \"ACEGIKMOQ\":\n","            facts.append(f\"{fact_prefix}_A\" if i % 2 == 0 else f\"{fact_prefix}_B\")\n","        else:\n","            facts.append(f\"{fact_prefix + 1}_A\" if i % 2 == 0 else f\"{fact_prefix + 1}_B\")\n","    parsed_data[\"fact\"] = facts\n","\n","    # Reorder columns to match the desired output\n","    final_parsed = parsed_data[[\"well_id\", \"sample_id\", \"fact\", \"name_long\", \"cells\", \"ref\"]]\n","\n","    # Save the final parsed data to the output file\n","    final_parsed.to_csv(output_file, sep=\"\\t\", index=False, na_rep=\"NA\")\n","    print(f\"Final parsed file saved to {output_file}\")\n","\n","# List of parsed files to process\n","parsed_files = [\n","    \"parsed_1AB2AB_t1.txt\",\n","    \"parsed_1AB2AB_t2.txt\",\n","    \"parsed_3AB4AB_t1.txt\",\n","    \"parsed_3AB4AB_t2.txt\",\n","    \"parsed_5AB6AB_t1.txt\",\n","    \"parsed_5AB6AB_t2.txt\"\n","]\n","\n","# Plate layout file\n","plate_layout_file = \"plate_layout_FA02.txt\"\n","\n","# Loop over the files and process each\n","for parsed_file in parsed_files:\n","    output_file = f\"final_{parsed_file}\"  # Output directly to the current directory\n","    transform_parsed_to_final(parsed_file, plate_layout_file, output_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1770187424495,"user":{"displayName":"Shreyas Pai","userId":"15597218115471730073"},"user_tz":300},"id":"l2EVkPW3DXiF","outputId":"db550f39-5790-4ba2-cb61-6e27bc346cc4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copied final_parsed_1AB2AB_t1.txt to FA02_1AB2AB_t1.txt\n","Copied final_parsed_1AB2AB_t2.txt to FA02_1AB2AB_t2.txt\n","Copied final_parsed_3AB4AB_t1.txt to FA02_3AB4AB_t1.txt\n","Copied final_parsed_3AB4AB_t2.txt to FA02_3AB4AB_t2.txt\n","Copied final_parsed_5AB6AB_t1.txt to FA02_5AB6AB_t1.txt\n","Copied final_parsed_5AB6AB_t2.txt to FA02_5AB6AB_t2.txt\n"]}],"source":["## renaming final_parsed_xxxxxx_t1/2.txt -> FA02_xxxxxx_t1/2.txt\n","\n","import os\n","\n","# List of files to copy\n","files_to_copy = [\n","    \"final_parsed_1AB2AB_t1.txt\",\n","    \"final_parsed_1AB2AB_t2.txt\",\n","    \"final_parsed_3AB4AB_t1.txt\",\n","    \"final_parsed_3AB4AB_t2.txt\",\n","    \"final_parsed_5AB6AB_t1.txt\",\n","    \"final_parsed_5AB6AB_t2.txt\"\n","]\n","\n","# Loop over the files and copy each with the new name\n","for file in files_to_copy:\n","    new_file_name = file.replace(\"final_parsed_\", \"FA02_\")\n","    with open(file, \"rb\") as src, open(new_file_name, \"wb\") as dest:\n","        dest.write(src.read())\n","    print(f\"Copied {file} to {new_file_name}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":86,"status":"ok","timestamp":1770187424580,"user":{"displayName":"Shreyas Pai","userId":"15597218115471730073"},"user_tz":300},"id":"HdcSKdQnDat7","outputId":"a16581b2-c25e-47b8-aed4-c18eaa4d0510"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data saved to FA02_parsed_fitness_data.txt\n"]}],"source":["## FA02_xxxxxx_t1/2.txt -> FA02_parsed_fitness_data.txt\n","\n","# Fitness estimation function\n","def fit_est(f1, f0, t):\n","    \"\"\"\n","    Calculate the fitness estimate using the given formula.\n","    \"\"\"\n","    return (1 / t) * (np.log(f1 / (1 - f1)) - np.log(f0 / (1 - f0)))\n","\n","# Define file paths for time 1 and time 2\n","path_t1 = \"FA02*t1.txt\"\n","path_t2 = \"FA02*t2.txt\"\n","\n","# Read all time 1 and time 2 files\n","files_t1 = sorted(glob.glob(path_t1))\n","files_t2 = sorted(glob.glob(path_t2))\n","\n","data_t1 = [pd.read_csv(file, sep=\"\\t\") for file in files_t1]\n","data_t2 = [pd.read_csv(file, sep=\"\\t\") for file in files_t2]\n","\n","# Combine all files into single DataFrames\n","t1_combined = pd.concat(data_t1, ignore_index=True)\n","t2_combined = pd.concat(data_t2, ignore_index=True)\n","\n","# Add generation as a factor column\n","t1_combined['t'] = 1\n","t2_combined['t'] = 10\n","\n","# Merge time 1 and time 2 data on common columns\n","data_merged = pd.merge(t1_combined, t2_combined, on=['well_id', 'sample_id', 'fact'], suffixes=('_t1', '_t2'))\n","\n","# Drop unnecessary columns\n","data_merged = data_merged.drop(columns=[col for col in data_merged if 'name_long' in col or col in ['t_t1', 't_t2']])\n","\n","# Rename columns\n","data_merged.columns = ['well_id', 'sample_id', 'fact', 'cells_t1', 'ref_t1', 'cells_t2', 'ref_t2']\n","\n","# Filter out rows where sample_id is 'BLANK'\n","data_filtered = data_merged[data_merged['sample_id'] != 'BLANK'].copy()\n","\n","# Add environment and replicate columns\n","data_filtered.loc[:, 'env'] = data_filtered['fact'].str.split('_').str[0]\n","data_filtered.loc[:, 'rep'] = data_filtered['fact'].str.split('_').str[1]\n","\n","# Add regime column\n","data_filtered.loc[:, 'regime'] = pd.NA\n","data_filtered.loc[data_filtered['sample_id'].str.contains('a1.1', na=False), 'regime'] = 'asexual'\n","data_filtered.loc[data_filtered['sample_id'].str.contains('a3', na=False), 'regime'] = 'sexual'\n","\n","# Flag ancestors, blanks, and references\n","data_filtered.loc[:, 'anc'] = np.where(data_filtered['sample_id'].str.contains('MJM64'), 1, 0)\n","data_filtered.loc[:, 'blank'] = np.where(data_filtered['sample_id'] == 'BLANK', 1, 0)\n","data_filtered.loc[:, 'ref'] = np.where(data_filtered['sample_id'].str.contains('REF_ALONE'), 1, 0)\n","\n","# Add kk_well_id column\n","data_filtered.loc[:, 'kk_well_id'] = data_filtered['sample_id'].str.split('_').str[1]\n","data_filtered.loc[(data_filtered['anc'] == 1) | (data_filtered['ref'] == 1), 'kk_well_id'] = np.nan\n","\n","# Replace environment mapping using .loc\n","environment_mapping = {\n","    '1': 'FLC4', '2': 'SC30C', '3': 'SC37C', '4': 'lowP', '5': 'SC_pH7.3', '6': 'SC_0.2M_NaCl'\n","}\n","data_filtered.loc[:, 'env'] = data_filtered['env'].replace(environment_mapping)\n","\n","# Set plate and assay columns using .loc\n","#data_filtered.loc[:, 'plate'] = data_filtered['env'] + '_' + data_filtered['rep']\n","data_filtered.loc[:, 'assay'] = 'FA02'\n","\n","# Rename 'fact' column without inplace modification\n","data_filtered = data_filtered.rename(columns={'fact': 'plate'})\n","\n","data_filtered.loc[:, 's_hat'] = fit_est(\n","    f0=(data_filtered['cells_t1'] - data_filtered['ref_t1']) / data_filtered['cells_t1'],\n","    f1=(data_filtered['cells_t2'] - data_filtered['ref_t2']) / data_filtered['cells_t2'],\n","    t=10\n",")\n","\n","data_filtered.loc[data_filtered['ref'] == 1, 's_hat'] = np.nan\n","\n","# Assign 'kk_pop_id' with .loc\n","data_filtered.loc[:, 'kk_pop_id'] = None\n","data_filtered.loc[(data_filtered['regime'] == 'sexual') & (data_filtered['anc'] == 0) & (data_filtered['ref'] == 0), 'kk_pop_id'] = \\\n","    'a3_' + data_filtered['kk_well_id']\n","data_filtered.loc[(data_filtered['regime'] == 'asexual') & (data_filtered['anc'] == 0) & (data_filtered['ref'] == 0), 'kk_pop_id'] = \\\n","    'a1.1_' + data_filtered['kk_well_id']\n","\n","# Relevant columns for export\n","columns_to_export = ['kk_well_id', 'assay', 'regime', 'plate', 'env', 'rep', 'anc', 'ref', 's_hat', 'kk_pop_id']\n","data_export = data_filtered[columns_to_export]\n","\n","def format_row(row):\n","    \"\"\"\n","    Formats each row:\n","    - Strings are quoted unless \"NA\".\n","    - Missing values are replaced with NA.\n","    \"\"\"\n","    return [\n","        \"NA\" if pd.isnull(val) or val == \"nan\" else (f'\"{val}\"' if isinstance(val, str) else str(val))\n","        for val in row\n","    ]\n","\n","# Save the data to a .txt file with correct formatting\n","with open(\"FA02_parsed_fitness_data.txt\", \"w\") as f:\n","    # Write the header with proper quoting\n","    f.write(\",\".join([f'\"{col}\"' for col in columns_to_export]) + \"\\n\")\n","    # Write the data rows\n","    for _, row in data_export.iterrows():\n","        formatted_row = format_row(row)\n","        f.write(\",\".join(formatted_row) + \"\\n\")\n","\n","print(\"Data saved to FA02_parsed_fitness_data.txt\")"]},{"cell_type":"markdown","metadata":{"id":"_3wZEJ7RGCOQ"},"source":["# Combining Fitness Data\n","\n","Going from `FA01_parsed_fitness_data.txt` and `FA01_parsed_fitness_data.txt` to `parsed_pop_fitness_data.csv`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1078,"status":"ok","timestamp":1770187427214,"user":{"displayName":"Shreyas Pai","userId":"15597218115471730073"},"user_tz":300},"id":"82T3gGTBYZSv","outputId":"be3f7886-a49f-458e-b308-95f012e39332"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n","  warnings.warn(msg, ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/statsmodels/regression/mixed_linear_model.py:1634: UserWarning: Random effects covariance is singular\n","  warnings.warn(msg)\n","/usr/local/lib/python3.12/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n","  warnings.warn(\"Maximum Likelihood optimization failed to \"\n","/usr/local/lib/python3.12/dist-packages/statsmodels/regression/mixed_linear_model.py:2206: ConvergenceWarning: MixedLM optimization failed, trying a different optimizer may help.\n","  warnings.warn(msg, ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/statsmodels/regression/mixed_linear_model.py:2218: ConvergenceWarning: Gradient optimization failed, |grad| = 0.544778\n","  warnings.warn(msg, ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n","  warnings.warn(msg, ConvergenceWarning)\n"]}],"source":["# --- load + combine (like R: YPD <- FA01, AWAY <- FA02, PD <- rbind) ---\n","fa01 = pd.read_csv(\"FA01_parsed_fitness_data.txt\")\n","fa02 = pd.read_csv(\"FA02_parsed_fitness_data.txt\")\n","PD = pd.concat([fa01, fa02], ignore_index=True)\n","\n","# R: filter(env != 'FLC4', ref == 0)\n","PD = PD[(PD[\"env\"] != \"FLC4\") & (PD[\"ref\"] == 0)].copy()\n","\n","def plate_effects_ml(df, fixed_formula):\n","    \"\"\"\n","    Mimic glmmTMB(..., family=gaussian) random intercepts:\n","      (1|kk_pop_id) + (1|plate)\n","    and extract the plate BLUPs. Use ML (reml=False) to match glmmTMB default.\n","    \"\"\"\n","    d = df.dropna(subset=[\"s_hat\", \"regime\", \"kk_pop_id\", \"plate\"]).copy()\n","    d[\"all\"] = 1  # single grouping level; use variance components for kk_pop_id and plate\n","    m = smf.mixedlm(\n","        f\"s_hat ~ {fixed_formula}\",\n","        d,\n","        groups=d[\"all\"],\n","        vc_formula={\"kk\": \"0 + C(kk_pop_id)\", \"plate\": \"0 + C(plate)\"},\n","        re_formula=\"0\",\n","    )\n","    r = m.fit(reml=False, method=\"lbfgs\", maxiter=500, disp=False)\n","\n","    re = r.random_effects[1]\n","    # keys look like: \"plate[C(plate)[YPD_A]]\" -> \"YPD_A\"\n","    return {k.split(\"[\")[-1].rstrip(\"]\"): v for k, v in re.items() if k.startswith(\"plate\")}\n","\n","# --- estimate plate effects per assay (exactly like the Rmd) ---\n","PD01 = PD[PD[\"assay\"] == \"FA01\"]\n","PD02 = PD[PD[\"assay\"] == \"FA02\"]\n","\n","# R: FA01 model: s_hat ~ regime + (1|kk_pop_id) + (1|plate)\n","plate1 = plate_effects_ml(PD01, \"C(regime)\")\n","\n","# R: FA02 model: s_hat ~ regime*env + (1|kk_pop_id) + (1|plate)\n","plate2 = plate_effects_ml(PD02, \"C(regime)*C(env)\")\n","\n","# R: PD2$s_hat_adj <- PD2$s_hat - PD2$plate_mean\n","PD[\"plate_mean\"] = PD[\"plate\"].map({**plate1, **plate2})\n","PD[\"s_hat_adj\"] = PD[\"s_hat\"] - PD[\"plate_mean\"]\n","\n","# R: MATa <- filter(PD2, anc==1) %>% group_by(env) %>% summarise(mu_s = mean(s_hat_adj), ...)\n","MATa = (\n","    PD[PD[\"anc\"] == 1]\n","    .groupby(\"env\", as_index=False)[\"s_hat_adj\"]\n","    .agg(mu_s=\"mean\", sd_s=\"std\")\n",")\n","\n","# R: PD3 <- merge(PD2, MATa, by='env'); PD3$fitness_gain <- PD3$s_hat_adj - PD3$mu_s\n","PD = PD.merge(MATa, on=\"env\", how=\"left\", sort=False)\n","PD[\"fitness_gain\"] = PD[\"s_hat_adj\"] - PD[\"mu_s\"]\n","\n","# R: PD4 <- filter(PD3, anc==0, ref==0) %>% group_by(kk_pop_id, env) %>% summarise(mean/sd of fitness_gain)\n","out = (\n","    PD[(PD[\"anc\"] == 0) & (PD[\"ref\"] == 0)]\n","    .groupby([\"kk_pop_id\", \"env\"], as_index=False)[\"fitness_gain\"]\n","    .agg(fitness_gain_mu_pop=\"mean\", fitness_gain_sd_pop=\"std\")\n",")\n","\n","out.to_csv(\"parsed_pop_fitness_data.csv\", index=False)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP3+xz/b6pit0zVFBzEjrnO","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
