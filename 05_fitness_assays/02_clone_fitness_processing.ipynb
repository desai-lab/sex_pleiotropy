{"cells":[{"cell_type":"code","execution_count":null,"id":"33208d53","metadata":{"id":"33208d53"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import csv\n","import math\n","import glob\n","import statsmodels.formula.api as smf"]},{"cell_type":"markdown","id":"e1a42df0","metadata":{"id":"e1a42df0"},"source":["# FA03\n","Fitness assays of clones in YPD, SC 30C, SC 37C, SC +NaCl, SC pH 7.3, SC -P.\n","\n","Going from `FA03_1AB2AB_T1.txt, FA03_1AB2AB_T2.txt, FA03_3AB4AB_T1.txt, FA03_3AB4AB_T2.txt, FA03_5AB6AB_T1.txt, FA03_5AB6AB_T2.txt` to `parsed_clone_fitness_data.csv`.\n","\n","Needs `plate_layout_FA03.txt` as well as `FA01_parsed_fitness_data.txt` and `FA02_parsed_fitness_data.txt` (outputs of `population_fitness_processing.ipynb`).\n"]},{"cell_type":"code","execution_count":null,"id":"6bdc6782","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bdc6782","executionInfo":{"status":"ok","timestamp":1770353515020,"user_tz":300,"elapsed":317,"user":{"displayName":"Shreyas Pai","userId":"15597218115471730073"}},"outputId":"32e784ac-f422-49ff-a132-93e1217ba48f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data saved to FA03_parsed_fitness_data.txt\n"]}],"source":["## FA03_*_T1/T2.txt -> FA03_parsed_fitness_data.txt  (FA03 plate layout aware)\n","\n","# Fitness estimation function (same form as FA01/FA02)\n","def fit_est(f1, f0, t):\n","    \"\"\"Calculate fitness estimate from two frequencies.\"\"\"\n","    # protect against 0/1\n","    eps = 1e-9\n","    f0 = np.clip(f0, eps, 1-eps)\n","    f1 = np.clip(f1, eps, 1-eps)\n","    return (1 / t) * (np.log(f1 / (1 - f1)) - np.log(f0 / (1 - f0)))\n","\n","# ---- input patterns ----\n","path_t1 = \"FA03_*_T1.txt\"\n","path_t2 = \"FA03_*_T2.txt\"\n","plate_layout_path = \"plate_layout_FA03.txt\"  # FA03-specific layout (different from FA01/FA02)\n","\n","# ---- load all T1/T2 files ----\n","files_t1 = sorted(glob.glob(path_t1))\n","files_t2 = sorted(glob.glob(path_t2))\n","\n","data_t1 = [pd.read_csv(file, sep=\"\\t\") for file in files_t1]\n","data_t2 = [pd.read_csv(file, sep=\"\\t\") for file in files_t2]\n","\n","t1_combined = pd.concat(data_t1, ignore_index=True)\n","t2_combined = pd.concat(data_t2, ignore_index=True)\n","\n","# Add generation factor columns (matches FA02 style; not kept)\n","t1_combined['t'] = 1\n","t2_combined['t'] = 10\n","\n","# Merge time 1 and time 2 data on common columns\n","data_merged = pd.merge(\n","    t1_combined, t2_combined,\n","    on=['well_id', 'sample_id', 'fact'],\n","    suffixes=('_t1', '_t2')\n",")\n","\n","# Drop extra columns (name_long) and temp generation columns\n","drop_cols = [c for c in data_merged.columns if 'name_long' in c] + ['t_t1', 't_t2']\n","drop_cols = [c for c in drop_cols if c in data_merged.columns]\n","data_merged = data_merged.drop(columns=drop_cols)\n","\n","# Rename columns to explicit T1/T2 names (FA03 uses 'dark' not 'ref')\n","data_merged.columns = [\n","    'well_id', 'sample_id', 'fact',\n","    'cells_t1', 'dark_t1',\n","    'cells_t2', 'dark_t2'\n","]\n","\n","# Filter out blanks (keep REF and ancestors)\n","data_filtered = data_merged[data_merged['sample_id'] != 'BLANK'].copy()\n","\n","# --- FA03 plate layout handling ---\n","# FA03 has a different layout; use it to (a) validate sample_id vs expected, (b) fill any missing IDs.\n","def read_plate_layout(layout_file):\n","    \"\"\"Read plate_layout_FA03.txt into a dict mapping well_id (A01..H12) -> expected sample_id.\"\"\"\n","    with open(layout_file, 'r') as fh:\n","        lines = [l.strip() for l in fh if l.strip()]\n","    # split on whitespace or tabs; infer ncols from first row\n","    first = re.split(r\"\\s+|\\t\", lines[0])\n","    ncols = len(first)\n","    rows = [chr(ord('A') + i) for i in range(8)]\n","    cols = list(range(1, ncols + 1))\n","    mapping = {}\n","    for r_idx, line in enumerate(lines[:8]):\n","        entries = re.split(r\"\\s+|\\t\", line)\n","        for c_idx, entry in enumerate(entries):\n","            mapping[f\"{rows[r_idx]}{cols[c_idx]:02d}\"] = entry\n","    return mapping\n","\n","layout_map = read_plate_layout(plate_layout_path)\n","\n","# expected sample_id from layout\n","data_filtered.loc[:, 'layout_sample_id'] = data_filtered['well_id'].map(layout_map)\n","\n","# If sample_id is missing, fill from layout. If it disagrees with layout, keep file sample_id but flag.\n","data_filtered.loc[:, 'layout_mismatch'] = (\n","    data_filtered['layout_sample_id'].notna() &\n","    (data_filtered['sample_id'] != data_filtered['layout_sample_id'])\n",")\n","data_filtered.loc[data_filtered['sample_id'].isna(), 'sample_id'] = data_filtered.loc[data_filtered['sample_id'].isna(), 'layout_sample_id']\n","\n","# --- normalize sample_id to match prior parsing ---\n","def normalize_sample_id(s):\n","    s = str(s).strip()\n","    if s == \"REF\":\n","        return \"REF\"\n","    if s.startswith(\"MJM\"):\n","        return s   # keep MJM##_MATa / MJM##_MATalpha exactly as-is\n","    # force A1.1/A3 tokens into consistent case\n","    s = s.replace(\"A1.1\", \"a1.1\")\n","    s = re.sub(r'_(A3)_', r'_a3_', s)\n","    parts = s.split(\"_\")\n","    if len(parts) >= 2:\n","        parts[1] = parts[1].lower()\n","    if len(parts) >= 3:\n","        parts[2] = parts[2].upper()\n","    return \"_\".join(parts)\n","\n","data_filtered.loc[:, 'sample_id'] = data_filtered['sample_id'].map(normalize_sample_id)\n","\n","# Add environment and replicate columns\n","data_filtered.loc[:, 'env'] = data_filtered['fact'].str.split('_').str[0]\n","data_filtered.loc[:, 'rep'] = data_filtered['fact'].str.split('_').str[1]\n","\n","# Add regime column\n","data_filtered.loc[:, 'regime'] = pd.NA\n","data_filtered.loc[data_filtered['sample_id'].str.contains('a1.1', na=False), 'regime'] = 'asexual'\n","data_filtered.loc[data_filtered['sample_id'].str.contains('a3', na=False), 'regime'] = 'sexual'\n","\n","# Flag ancestors and references (FA03 ancestors contain 'MJM'; REF is exactly 'REF')\n","data_filtered.loc[:, 'anc'] = np.where(data_filtered['sample_id'].str.match(r'^MJM\\d+_MATa$', na=False), 1, 0)\n","data_filtered.loc[:, 'ref'] = np.where(data_filtered['sample_id'] == 'REF', 1, 0)\n","\n","# Add kk_well_id column (FA03 sample_id pattern: <batch>_<regime>_<WELL>_<clone>)\n","data_filtered.loc[:, 'kk_well_id'] = data_filtered['sample_id'].str.split('_').str[2]\n","data_filtered.loc[(data_filtered['anc'] == 1) | (data_filtered['ref'] == 1), 'kk_well_id'] = np.nan\n","\n","# Replace environment mapping (match prior FA03 parsing)\n","environment_mapping = {\n","    'YPD': 'YPD',\n","    'SC': 'SC30C',\n","    'SC37': 'SC37C',\n","    'lowP': 'lowP',\n","    'pH7.3': 'SC_pH7.3',\n","    'NaCl': 'SC_0.2M_NaCl'\n","}\n","data_filtered.loc[:, 'env'] = data_filtered['env'].replace(environment_mapping)\n","\n","# Set assay and plate columns (match FA02 style)\n","data_filtered.loc[:, 'assay'] = 'FA03'\n","data_filtered = data_filtered.rename(columns={'fact': 'plate'})\n","\n","# compute s_hat using dark/cells fractions (FA03)\n","data_filtered.loc[:, 's_hat'] = fit_est(\n","    f0=(data_filtered['dark_t1']) / data_filtered['cells_t1'],\n","    f1=(data_filtered['dark_t2']) / data_filtered['cells_t2'],\n","    t=10\n",")\n","data_filtered.loc[:, 's_hat_for_analysis'] = data_filtered['s_hat'].copy()\n","data_filtered.loc[data_filtered['ref'] == 1, 's_hat'] = np.nan\n","\n","# Assign kk_pop_id (match FA02 logic, but FA03 well ID is in kk_well_id)\n","data_filtered.loc[:, 'kk_pop_id'] = None\n","data_filtered.loc[(data_filtered['regime'] == 'sexual') & (data_filtered['anc'] == 0) & (data_filtered['ref'] == 0), 'kk_pop_id'] = \\\n","    'a3_' + data_filtered['kk_well_id']\n","data_filtered.loc[(data_filtered['regime'] == 'asexual') & (data_filtered['anc'] == 0) & (data_filtered['ref'] == 0), 'kk_pop_id'] = \\\n","    'a1.1_' + data_filtered['kk_well_id']\n","data_filtered.loc[(data_filtered['ref'] == 1), 'kk_pop_id'] = 'REF'\n","data_filtered.loc[(data_filtered['anc'] == 1), 'kk_pop_id'] = data_filtered.loc[(data_filtered['anc'] == 1), 'sample_id']\n","\n","# Ensure MATalpha control (e.g., MJM36_MATalpha) gets a stable kk_pop_id (it has regime = NA)\n","mask_matalpha = (\n","    data_filtered['sample_id'].str.match(r'^MJM\\d+_MATalpha$', na=False) &\n","    (data_filtered['anc'] == 0) &\n","    (data_filtered['ref'] == 0)\n",")\n","data_filtered.loc[mask_matalpha, 'kk_pop_id'] = data_filtered.loc[mask_matalpha, 'sample_id']\n","\n","# Relevant columns for export (keep sample_id, plus layout QC fields at end)\n","columns_to_export = [\n","    'kk_well_id','sample_id','assay','regime','plate','env','rep','anc','ref','s_hat','kk_pop_id'\n","]\n","data_export = data_filtered[columns_to_export]\n","\n","def format_row(row):\n","    \"\"\"Format like the existing FAxx_parsed_fitness_data.txt outputs: quote strings, NA for missing.\"\"\"\n","    out = []\n","    for val in row:\n","        if pd.isnull(val) or val == \"nan\":\n","            out.append(\"NA\")\n","        elif isinstance(val, str):\n","            out.append(f'\"{val}\"')\n","        else:\n","            out.append(str(val))\n","    return out\n","\n","# Save the data to a .txt file with correct formatting\n","out_txt = \"FA03_parsed_fitness_data.txt\"\n","with open(out_txt, \"w\") as f:\n","    f.write(\",\".join([f'\"{col}\"' for col in columns_to_export]) + \"\\n\")\n","    for _, row in data_export.iterrows():\n","        f.write(\",\".join(format_row(row)) + \"\\n\")\n","\n","print(f\"Data saved to {out_txt}\")\n","#print(f\"Layout mismatches flagged: {int(data_export['layout_mismatch'].sum())}\")"]},{"cell_type":"code","source":["## FA03_parsed_fitness_data.txt -> parsed_clone_fitness_data_FA03.csv\n","\n","# ---------- helper: extract plate BLUPs (like your plate_effects_ml) ----------\n","def plate_effects_ml(df, fixed_formula):\n","    d = df.dropna(subset=[\"s_hat\", \"regime\", \"kk_pop_id\", \"plate\"]).copy()\n","    d[\"all\"] = 1\n","    m = smf.mixedlm(\n","        f\"s_hat ~ {fixed_formula}\",\n","        d,\n","        groups=d[\"all\"],\n","        vc_formula={\"kk\": \"0 + C(kk_pop_id)\", \"plate\": \"0 + C(plate)\"},\n","        re_formula=\"0\",\n","    )\n","    r = m.fit(reml=False, method=\"lbfgs\", maxiter=500, disp=False)\n","\n","    re = r.random_effects[1]\n","    return {k.split(\"[\")[-1].rstrip(\"]\"): v for k, v in re.items() if k.startswith(\"plate\")}\n","\n","# ---------- 1) compute mu_s(env) from FA01 + FA02 (SP does this) ----------\n","fa01 = pd.read_csv(\"FA01_parsed_fitness_data.txt\")\n","fa02 = pd.read_csv(\"FA02_parsed_fitness_data.txt\")\n","PD = pd.concat([fa01, fa02], ignore_index=True)\n","\n","# SP: filter(env != 'FLC4', ref == 0)\n","PD = PD[(PD[\"env\"] != \"FLC4\") & (PD[\"ref\"] == 0)].copy()\n","\n","PD01 = PD[PD[\"assay\"] == \"FA01\"].copy()\n","PD02 = PD[PD[\"assay\"] == \"FA02\"].copy()\n","\n","plate1 = plate_effects_ml(PD01, \"C(regime)\")\n","plate2 = plate_effects_ml(PD02, \"C(regime)*C(env)\")\n","\n","PD[\"plate_mean\"] = PD[\"plate\"].map({**plate1, **plate2})\n","PD[\"s_hat_adj\"] = PD[\"s_hat\"] - PD[\"plate_mean\"]\n","\n","# SP: MATa baseline by env\n","MATa = (\n","    PD[PD[\"anc\"] == 1]\n","    .groupby(\"env\", as_index=False)[\"s_hat_adj\"]\n","    .agg(mu_s=\"mean\")\n",")\n","\n","# ---------- 2) load FA03 parsed fitness, merge mu_s, fit FA03 plate model ----------\n","CD = pd.read_csv(\"FA03_parsed_fitness_data.txt\", quotechar='\"')\n","\n","# merge mu_s onto FA03 (CD2 in Rmd)\n","CD2 = CD.merge(MATa, on=\"env\", how=\"left\", sort=False)\n","\n","# Fit FA03 plate effects like R: s_hat ~ regime*env + (1|kk_pop_id) + (1|plate)\n","# (statsmodels will drop NA rows automatically for fitting)\n","fit_df = CD2.dropna(subset=[\"s_hat\", \"regime\", \"kk_pop_id\", \"plate\"]).copy()\n","fit_df[\"all\"] = 1\n","\n","m = smf.mixedlm(\n","    \"s_hat ~ C(regime)*C(env)\",\n","    fit_df,\n","    groups=fit_df[\"all\"],\n","    vc_formula={\"kk\": \"0 + C(kk_pop_id)\", \"plate\": \"0 + C(plate)\"},\n","    re_formula=\"0\",\n",")\n","r = m.fit(reml=False, method=\"lbfgs\", maxiter=500, disp=False)\n","\n","re = r.random_effects[1]\n","plate_effect_FA03 = {\n","    k.split(\"[\")[-1].rstrip(\"]\"): v\n","    for k, v in re.items()\n","    if k.startswith(\"plate\")\n","}\n","\n","# ---------- 3) compute unaggregated clone fitness_gain (CD3 in Rmd) ----------\n","CD3 = CD2.copy()\n","CD3[\"plate_means\"] = CD3[\"plate\"].map(plate_effect_FA03)\n","CD3[\"fitness_gain\"] = CD3[\"s_hat\"] - CD3[\"plate_means\"] - CD3[\"mu_s\"]\n","\n","# # write unaggregated (matches Rmd intent)\n","# CD3.to_csv(\"parsed_unagg_clone_fitness_data_FA03.csv\", index=False)\n","\n","# ---------- 4) aggregate to final clone CSV (matches fitness_assay_processing.R) ----------\n","CD4 = CD3[(CD3[\"anc\"] == 0) & (CD3[\"ref\"] == 0)].copy()\n","\n","# Capture first-appearance order of each clone group (R-like group order)\n","GROUP_KEY = [\"kk_pop_id\", \"sample_id\", \"env\"]\n","group_order = CD4[GROUP_KEY].drop_duplicates(keep=\"first\").reset_index(drop=True)\n","group_order[\"__order\"] = np.arange(len(group_order))\n","\n","clone_summary = (\n","    CD4.groupby([\"kk_pop_id\", \"sample_id\", \"env\"], as_index=False, sort=False)[\"fitness_gain\"]\n","    .agg(\n","        fitness_gain_avg=lambda x: np.nanmean(x),\n","        fitness_gain_sd=lambda x: np.nanstd(x, ddof=1),\n","    )\n",")\n","\n","# attach regime/anc/ref like the R aggregation does (first())\n","meta = CD4.groupby([\"kk_pop_id\", \"sample_id\", \"env\"], as_index=False, sort=False).agg(\n","    regime=(\"regime\", \"first\"),\n","    anc=(\"anc\", \"first\"),\n","    ref=(\"ref\", \"first\"),\n",")\n","clone_summary = clone_summary.merge(meta, on=[\"kk_pop_id\", \"sample_id\", \"env\"], how=\"left\")\n","\n","clone_summary[\"regime\"] = (clone_summary[\"regime\"].astype(\"string\").str.strip().replace(\"\", pd.NA))\n","\n","# Apply the R-like group order\n","clone_summary = (\n","    clone_summary\n","    .merge(group_order, on=GROUP_KEY, how=\"left\")\n","    .sort_values(\"__order\", kind=\"mergesort\")\n","    .drop(columns=\"__order\")\n",")\n","\n","# reorder columns like SP clone output\n","clone_summary = clone_summary[\n","    [\"sample_id\",\"env\",\"regime\",\"anc\",\"ref\",\"kk_pop_id\",\"fitness_gain_avg\",\"fitness_gain_sd\"]\n","]\n","\n","env_order = [\"lowP\", \"SC_0.2M_NaCl\", \"SC_pH7.3\", \"SC30C\", \"SC37C\", \"YPD\"]\n","clone_summary[\"env\"] = pd.Categorical(clone_summary[\"env\"], categories=env_order, ordered=True)\n","clone_summary = clone_summary.sort_values([\"sample_id\", \"env\"], kind=\"mergesort\")\n","\n","clone_summary.index = np.arange(1, len(clone_summary) + 1)\n","clone_summary.to_csv(\"parsed_clone_fitness_data_FA03.csv\", index=True, na_rep=\"NA\")\n","print(\"Wrote parsed_clone_fitness_data_FA03.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ll02nkPTL2uS","executionInfo":{"status":"ok","timestamp":1770353520416,"user_tz":300,"elapsed":5394,"user":{"displayName":"Shreyas Pai","userId":"15597218115471730073"}},"outputId":"50187c8c-5555-47cd-8692-7f9678d25b02"},"id":"ll02nkPTL2uS","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n","  warnings.warn(msg, ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/statsmodels/regression/mixed_linear_model.py:1634: UserWarning: Random effects covariance is singular\n","  warnings.warn(msg)\n","/usr/local/lib/python3.12/dist-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n","  warnings.warn(msg, ConvergenceWarning)\n","/usr/local/lib/python3.12/dist-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n","  warnings.warn(msg, ConvergenceWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Wrote parsed_unagg_clone_fitness_data_FA03.csv and parsed_clone_fitness_data_FA03.csv\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1wzO7z_PfbX-TflxbMde0fLAdhqVkSHI8","timestamp":1770353720072}]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}